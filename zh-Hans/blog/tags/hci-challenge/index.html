<!doctype html>
<html lang="zh-Hans" dir="ltr" class="blog-wrapper blog-tags-post-list-page plugin-blog plugin-id-default" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.0.0">
<title data-rh="true">5 ç¯‡åšæ–‡ å«æœ‰æ ‡ç­¾ã€ŒHCI challengeã€ | CaptchaAgent</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://docs.captchax.top/zh-Hans/img/CaptchaAgent-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://docs.captchax.top/zh-Hans/img/CaptchaAgent-social-card.jpg"><meta data-rh="true" property="og:url" content="https://docs.captchax.top/zh-Hans/blog/tags/hci-challenge"><meta data-rh="true" property="og:locale" content="zh_Hans"><meta data-rh="true" property="og:locale:alternate" content="en"><meta data-rh="true" name="docusaurus_locale" content="zh-Hans"><meta data-rh="true" name="docsearch:language" content="zh-Hans"><meta data-rh="true" property="og:title" content="5 ç¯‡åšæ–‡ å«æœ‰æ ‡ç­¾ã€ŒHCI challengeã€ | CaptchaAgent"><meta data-rh="true" name="docusaurus_tag" content="blog_tags_posts"><meta data-rh="true" name="docsearch:docusaurus_tag" content="blog_tags_posts"><link data-rh="true" rel="icon" href="/zh-Hans/img/favicon2.png"><link data-rh="true" rel="canonical" href="https://docs.captchax.top/zh-Hans/blog/tags/hci-challenge"><link data-rh="true" rel="alternate" href="https://docs.captchax.top/blog/tags/hci-challenge" hreflang="en"><link data-rh="true" rel="alternate" href="https://docs.captchax.top/zh-Hans/blog/tags/hci-challenge" hreflang="zh-Hans"><link data-rh="true" rel="alternate" href="https://docs.captchax.top/blog/tags/hci-challenge" hreflang="x-default"><link data-rh="true" rel="preconnect" href="https://63F28NXXKD-dsn.algolia.net" crossorigin="anonymous"><link rel="alternate" type="application/rss+xml" href="/zh-Hans/blog/rss.xml" title="CaptchaAgent RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/zh-Hans/blog/atom.xml" title="CaptchaAgent Atom Feed">



<link rel="search" type="application/opensearchdescription+xml" title="CaptchaAgent" href="/zh-Hans/opensearch.xml"><link rel="stylesheet" href="/zh-Hans/assets/css/styles.9bfbcf78.css">
<script src="/zh-Hans/assets/js/runtime~main.7997c639.js" defer="defer"></script>
<script src="/zh-Hans/assets/js/main.08484d60.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="è·³åˆ°ä¸»è¦å†…å®¹"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">è·³åˆ°ä¸»è¦å†…å®¹</a></div><nav aria-label="ä¸»å¯¼èˆª" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="åˆ‡æ¢å¯¼èˆªæ " aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/zh-Hans/"><div class="navbar__logo"><img src="/zh-Hans/img/logo.png" alt="CaptchaAgent" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/zh-Hans/img/logo.png" alt="CaptchaAgent" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">CaptchaAgent</b></a><a class="navbar__item navbar__link" href="/zh-Hans/docs/intro">Docs</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/zh-Hans/blog">Blog</a></div><div class="navbar__items navbar__items--right"><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link"><svg viewBox="0 0 24 24" width="20" height="20" aria-hidden="true" class="iconLanguage_nlXk"><path fill="currentColor" d="M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z"></path></svg>ç®€ä½“ä¸­æ–‡</a><ul class="dropdown__menu"><li><a href="/blog/tags/hci-challenge" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="en">English</a></li><li><a href="/zh-Hans/blog/tags/hci-challenge" target="_self" rel="noopener noreferrer" class="dropdown__link dropdown__link--active" lang="zh-Hans">ç®€ä½“ä¸­æ–‡</a></li></ul></div><a href="https://github.com/QIN2DIM/hcaptcha-challenger" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="åˆ‡æ¢æµ…è‰²/æš—é»‘æ¨¡å¼ï¼ˆå½“å‰ä¸ºæµ…è‰²æ¨¡å¼ï¼‰" aria-label="åˆ‡æ¢æµ…è‰²/æš—é»‘æ¨¡å¼ï¼ˆå½“å‰ä¸ºæµ…è‰²æ¨¡å¼ï¼‰" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"><button type="button" class="DocSearch DocSearch-Button" aria-label="æœç´¢"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 20 20"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">æœç´¢</span></span><span class="DocSearch-Button-Keys"></span></button></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_re4s thin-scrollbar" aria-label="æœ€è¿‘åšæ–‡å¯¼èˆª"><div class="sidebarItemTitle_pO2u margin-bottom--md">Recent posts</div><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zh-Hans/blog/vit-zero-shot-tasks-1">é›¶æ ·æœ¬å›¾åƒåˆ†ç±»ä»»åŠ¡</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zh-Hans/blog/draw-a-tight-bounding-box-around-the-x">åœ¨ç›®æ ‡å‘¨å›´ç”»ä¸€ä¸ªè¾¹ç•Œæ¡†</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zh-Hans/blog/please-click-on-the-x">åœ¨ç›®æ ‡ä¸Šç‚¹å‡»</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zh-Hans/blog/bot-revolution-2023">æœºå™¨é©å‘½ 2023</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zh-Hans/blog/can-dogs-smile">ç§€ç‹—åœ¨ç¬‘ï¼Ÿ</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/zh-Hans/blog/how-old-is-that-kitten">ä½ å®¶çŒ«çŒ«â€¦â€¦å‡ å²ï¼</a></li></ul></nav></aside><main class="col col--7" itemscope="" itemtype="https://schema.org/Blog"><header class="margin-bottom--xl"><h1>5 ç¯‡åšæ–‡ å«æœ‰æ ‡ç­¾ã€ŒHCI challengeã€</h1><a href="/zh-Hans/blog/tags">æŸ¥çœ‹æ‰€æœ‰æ ‡ç­¾</a></header><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="https://schema.org/BlogPosting"><meta itemprop="description" content="Preview"><meta itemprop="keywords" content="hcaptcha,captcha,YOLOv8,ResNet,object detection,image segmentation,bounding box,clip,vit,huggingface,llm,OpenAgents,automatic image annotation,CaptchaAgent,GPT,ONNX,hcaptca-challenger"><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/zh-Hans/blog/vit-zero-shot-tasks-1">é›¶æ ·æœ¬å›¾åƒåˆ†ç±»ä»»åŠ¡</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2023-10-22T00:00:00.000Z" itemprop="datePublished">2023å¹´10æœˆ22æ—¥</time> Â· <!-- -->é˜…è¯»éœ€ 7 åˆ†é’Ÿ</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://github.com/QIN2DIM" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://github.com/QIN2DIM.png" alt="QIN2DIM" itemprop="image"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/QIN2DIM" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">QIN2DIM</span></a></div><small class="avatar__subtitle" itemprop="description">Maintainer of CaptchaAgent</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><h2 class="anchor anchorWithStickyNavbar_LWe7" id="preview">Preview<a href="#preview" class="hash-link" aria-label="Previewçš„ç›´æ¥é“¾æ¥" title="Previewçš„ç›´æ¥é“¾æ¥">â€‹</a></h2>
<blockquote>
<p><a href="https://arxiv.org/abs/2103.00020" target="_blank" rel="noopener noreferrer">CLIP</a> is a multi-modal vision and language model. It can be used for image-text similarity and for zero-shot image classification. CLIP uses a ViT like transformer to get visual features and a causal language model to get the text features. Both the text and visual features are then projected to a latent space with identical dimension. The dot product between the projected image and text features is then used as a similar score.</p>
<p><a href="https://huggingface.co/docs/transformers/model_doc/clip" target="_blank" rel="noopener noreferrer">-- huggingface.co</a></p>
</blockquote>
<p><img loading="lazy" src="https://r2-datalake.echosec.top/blog-obs/2023/10/3958bc6e52d9c3fc1fdabaf17657db76.png" alt="CLIP" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="milestone">Milestone<a href="#milestone" class="hash-link" aria-label="Milestoneçš„ç›´æ¥é“¾æ¥" title="Milestoneçš„ç›´æ¥é“¾æ¥">â€‹</a></h2>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>ä¿¡æ¯</div><div class="admonitionContent_BuS1"><p>We merged a  <a href="https://github.com/QIN2DIM/hcaptcha-challenger/issues/858" target="_blank" rel="noopener noreferrer">feature #858</a> to the main branch of <a href="https://github.com/QIN2DIM/hcaptcha-challenger" target="_blank" rel="noopener noreferrer">hcaptcha-challenger</a> on October 22, 2023 to handle CAPTCHA via the CLIP image-text cross-modal model.</p></div></div>
<p>Previously, we trained and used the <a href="https://github.com/CaptchaAgent/hcaptcha-model-factory/blob/main/src/factories/resnet.py#L28" target="_blank" rel="noopener noreferrer">ResNet model</a> to handle the image classification challenge. The model network parameters are so small that our <a href="https://pytorch.org/tutorials/beginner/onnx/export_simple_model_to_onnx_tutorial.html" target="_blank" rel="noopener noreferrer">exported</a> ResNet <a href="https://onnxruntime.ai/" target="_blank" rel="noopener noreferrer">ONNX model</a> is only 294KB and we can still get over 80% correct in the binary classification task. This is more than enough for a CAPTCHA challenge with only nine images.</p>
<blockquote>
<p>But today, in 2023, there are so many key breakthroughs in <a href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=&amp;cad=rja&amp;uact=8&amp;ved=2ahUKEwiahvedm56CAxXEI0QIHQeMBLkQFnoECBkQAQ&amp;url=https%3A%2F%2Fwww.ibm.com%2Ftopics%2Fcomputer-vision&amp;usg=AOvVaw3K4rO-yHQ-RhOQjD0LbKZO&amp;opi=89978449" target="_blank" rel="noopener noreferrer">Computer Vision</a> that we can easily lift the accuracy to 98%+ on such simple tasks from the  CAPTCHAğŸ˜®.</p>
</blockquote>
<p>Thus, we also designed a <a href="https://github.com/CaptchaAgent/hcaptcha-model-factory/tree/main/automation" target="_blank" rel="noopener noreferrer">factory workflow</a> based on this, i.e., using the same network model design, but training models for different prompt scenarios on different batches of image data.</p>
<p>Although these small models can only handle binary classification tasks with a single target, we trade off an extreme performance experience, i.e., we go from training to iterating a model version in just a few minutes.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="automatic-image-annotation">Automatic image annotation<a href="#automatic-image-annotation" class="hash-link" aria-label="Automatic image annotationçš„ç›´  æ¥é“¾æ¥" title="Automatic image annotationçš„ç›´æ¥é“¾æ¥">â€‹</a></h2>
<div class="theme-admonition theme-admonition-note admonition_xJq3 alert alert--secondary"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>å¤‡æ³¨</div><div class="admonitionContent_BuS1"><p><a href="https://huggingface.co/tasks/zero-shot-image-classification" target="_blank" rel="noopener noreferrer">Zero shot image classification</a> is the task of classifying previously unseen classes during training of a model.</p></div></div>
<p><img loading="lazy" src="https://r2-datalake.echosec.top/blog-obs/2023/10/6dcf0cdc5089fa33fae236ab7ab4a951.png" alt="image-20231031022847507" class="img_ev3q"></p>
<p>We have been using CLIP in the factory workflow for <a href="https://en.wikipedia.org/wiki/Automatic_image_annotation" target="_blank" rel="noopener noreferrer">automatic image annotation</a> tasks since 2022, and at that time, the accuracy of <a href="https://github.com/mlfoundations/open_clip/blob/main/docs/openclip_classification_results.csv" target="_blank" rel="noopener noreferrer">the best CLIP model</a> in dealing with <a href="https://huggingface.co/tasks/zero-shot-image-classification" target="_blank" rel="noopener noreferrer">zero-shot image classification</a> was not satisfactory, but it was very useful for saving your time.</p>
<p>If I chose to manually categorize 1k images, it would probably take me at least 10 minutes and my nerves would be high the whole time. Oh and at the same time you&#x27;re going to need a processing environment that&#x27;s just right or you&#x27;re going to have a very slow process.</p>
<p>With CLIP, the same process takes 3 minutes. I only need to pay extremely little attention to samples that are misclassified or have low scores.</p>
<p><img loading="lazy" src="https://r2-datalake.echosec.top/blog-obs/2023/10/450ca5abdc2de3401c8f687d6952ffb2.jpg" alt="Default_Exaggerated_style_cartoon_where_a_girl_is_laboring_in_1_e5026ef7-6366-42b3-8215-6c9ca1887679_1" class="img_ev3q"></p>
<p>It may seem like a bit of a contradiction in terms. But if you&#x27;ve had this experience, you should know the difficulty of <u>multi-categorizing a bunch of cluttered images</u> is quite different for human attention than <u>identifying outliers in a bunch of images with similar overall characteristics</u>.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="dilemmas-and-breakthroughs">Dilemmas and breakthroughs<a href="#dilemmas-and-breakthroughs" class="hash-link" aria-label="Dilemmas and breakthroughsçš„ç›´æ¥é“¾æ¥" title="Dilemmas and breakthroughsçš„ç›´æ¥é“¾æ¥">â€‹</a></h2>
<p>At the time, we were only running CLIP on a local GPU server. we tried to run it on edge terminals and open up the inference interface, but how to export CLIP to ONNX and run inference tasks on terminals that don&#x27;t rely on pytorch and GPUs?</p>
<p>This question stumped me. So much so that as projects like <a href="https://github.com/jina-ai/clip-as-service" target="_blank" rel="noopener noreferrer">clip-as-service</a> started to gain traction in the industry, we tried the next best thing, which was to assume that there were enough devices in the edge network to run the CLIP model.ğŸ¥¹</p>
<p>Note that you don&#x27;t need much configuration to run the CLIP model, in the case of <code>RN50.openai</code> you only need up to 500MB of RAM to run it, and we&#x27;re just comparing it here to ResNet (294KB) from the previous section. That is, if we choose to design the CLIP model as a hot-swappable component, we can&#x27;t possibly require players using this program to be ready to download a 500MB model and read it frequently at any time.</p>
<p>With breakthroughs in computer vision research around the world, pre-trained models that score increasingly well are starting to emerge. <a href="https://github.com/mlfoundations/open_clip/blob/main/docs/openclip_results.csv" target="_blank" rel="noopener noreferrer">â†ªï¸benchmarks</a></p>
<blockquote>
<ul>
<li><a href="https://huggingface.co/docs/transformers/main/en/model_doc/clip" target="_blank" rel="noopener noreferrer">CLIP</a> supports zero-shot image classification with prompts. Given an image, you can prompt the CLIP model with a natural language query like &quot;a picture of <!-- -->&quot;. The expectation is to get the category label as the answer.</li>
<li><a href="https://huggingface.co/docs/transformers/main/en/model_doc/owlvit" target="_blank" rel="noopener noreferrer">OWL-ViT</a> allows zero-shot object detection conditional on language and one-shot object detection conditional on image. This means that you can detect objects in a single image even if the underlying model didn&#x27;t learn to detect them during training! You can refer to <a href="https://github.com/huggingface/notebooks/tree/main/examples#:~:text=zeroshot_object_detection_with_owlvit.ipynb" target="_blank" rel="noopener noreferrer">this notebook</a> to learn more.</li>
<li><a href="https://huggingface.co/docs/transformers/model_doc/clipseg" target="_blank" rel="noopener noreferrer">CLIPSeg</a> supports zero-shot image segmentation conditional on language and one-shot image segmentation conditional on image. This means you can segment objects in a single image even if the underlying model didn&#x27;t learn to segment them during training! You can refer to <a href="https://huggingface.co/blog/clipseg-zero-shot" target="_blank" rel="noopener noreferrer">this blog post</a> that illustrates this idea. <a href="https://huggingface.co/docs/transformers/model_doc/groupvit" target="_blank" rel="noopener noreferrer">GroupViT</a> also supports zero-shot segmentation.</li>
<li><a href="https://huggingface.co/docs/transformers/main/en/model_doc/xclip" target="_blank" rel="noopener noreferrer">X-CLIP</a> Demonstrates zero-shot generalization to video. To be precise, it supports zero-shot video classification. Check out <a href="https://github.com/NielsRogge/Transformers-Tutorials/blob/master/X-CLIP/Zero_shot_classify_a_YouTube_video_with_X_CLIP.ipynb" target="_blank" rel="noopener noreferrer">this notebook</a> for more details.</li>
</ul>
</blockquote>
<p>At the same time, the higher the model&#x27;s score, the larger its number of parameters usually is and the more memory it takes up. This also predisposes CLIP to be a model better suited to deal with decision-based tasks, where it is relatively too slow to respond. The fastest open-source CLIP models available with just the right number of parameters still don&#x27;t process fast enough on the CPU.</p>
<p>In the case of the CAPTCHA, deploying and using CLIP on user endpoints seems anachronistic. This is because for CAPTCHA, the challenges involving the CV and NLP parts do not yet fully utilize the strengths of the cross-modal model. It is clear that the CLIP model is a performance overflow for CAPTCHA available worldwide.</p>
<p>That is, it specializes in a combination of abilities we don&#x27;t yet need, but for which it can do things, we have better options in the moment.</p>
<p>It is entirely possible to opt for more targeted solutions, such as the use of small-volume models (e.g. <a href="https://huggingface.co/timm/mobilenetv3_large_100.ra_in1k" target="_blank" rel="noopener noreferrer">mobilenetv3</a> and <a href="https://github.com/baaivision/EVA/tree/master/EVA-02/asuka" target="_blank" rel="noopener noreferrer">EVA-02</a>) that are only valid for a specific task, but have an accuracy rate that far exceeds that of CLIP. <a href="https://github.com/huggingface/pytorch-image-models/blob/main/results/benchmark-infer-amp-nchw-pt113-cu117-rtx3090.csv" target="_blank" rel="noopener noreferrer">â†ªï¸benchmarks</a></p>
<p><img loading="lazy" src="https://r2-datalake.echosec.top/blog-obs/2023/10/d101ab93a2687bf69889c2b38cc62541.png" alt="radar" class="img_ev3q"><img loading="lazy" src="https://r2-datalake.echosec.top/blog-obs/2023/10/8bf027fff3915204cd10e2bb7f2bc4d6.png" alt="summary_tab" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="scenarios-for-the-clip-onnx">Scenarios for the CLIP-ONNX<a href="#scenarios-for-the-clip-onnx" class="hash-link" aria-label="Scenarios for the CLIP-ONNXçš„ç›´æ¥é“¾æ¥" title="Scenarios for the CLIP-ONNXçš„ç›´æ¥é“¾æ¥">â€‹</a></h2>
<p>As we mentioned earlier, for the automatic image annotation and remote service scenarios, we don&#x27;t need to export the model to ONNX, and we should instead make full use of the GPU (if available) to improve performance.</p>
<p>It is an inevitable trend that model parameters are getting larger and larger.  So based on the present and looking to the future, what are the possible application scenarios for CLIP-ONNX?</p>
<p>ğŸš§</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="the-generation-of-the-llm">The generation of the LLM<a href="#the-generation-of-the-llm" class="hash-link" aria-label="The generation of the LLMçš„ç›´æ¥é“¾æ¥" title="The generation of the LLMçš„ç›´æ¥é“¾æ¥">â€‹</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="visual-question-answering">Visual Question Answering<a href="#visual-question-answering" class="hash-link" aria-label="Visual Question Answeringçš„ç›´æ¥é“¾æ¥" title="Visual Question Answeringçš„ç›´æ¥é“¾æ¥">â€‹</a></h3>
<div class="theme-admonition theme-admonition-note admonition_xJq3 alert alert--secondary"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>å¤‡æ³¨</div><div class="admonitionContent_BuS1"><p><a href="https://huggingface.co/tasks/visual-question-answering" target="_blank" rel="noopener noreferrer">Visual Question Answering</a> is the task of answering open-ended questions based on an image. They output natural language responses to natural language questions.</p></div></div>
<p><img loading="lazy" src="https://r2-datalake.echosec.top/blog-obs/2023/10/2a915379c573c45234bf03f9150a8133.png" alt="image-20231031052543037" class="img_ev3q"></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="llm-openagents">LLM OpenAgents<a href="#llm-openagents" class="hash-link" aria-label="LLM OpenAgentsçš„ç›´æ¥é“¾æ¥" title="LLM OpenAgentsçš„ç›´æ¥é“¾æ¥">â€‹</a></h3>
<p>XLang Agents are Large Language Model-powered(LLM-powered) Agents, aiming to utilize a range of tools to enhance their capabilities, serving as user-centric intelligent agents. Currently, the XLang Agents supports three different agents focusing on different application scenarios, including:</p>
<ul>
<li><strong>Data Agent</strong>: The Data Agent is equipped with data-related tools, allowing it to efficiently search, handle and manipulate and visualize data. It is proficient in writing and executing code, enabling various data-related tasks.</li>
<li><strong>Plugins Agent</strong>: The Plugins Agent boasts integration with over 200 plugins from third-party sources. These plugins are carefully selected to cater to various aspects of your daily life scenarios. By leveraging these plugins, the agent can assist you with a wide range of tasks and activities.</li>
<li><strong>Web Agent</strong>: The Web Agent harnesses the power of a chrome extension to navigate and explore websites automatically. This agent streamlines the web browsing experience, making it easier for you to find relevant information, access desired resources, and so on.</li>
</ul>
<p><img loading="lazy" src="https://r2-datalake.echosec.top/blog-obs/2023/10/1960d1a546180a884df4a3aa7fd03d9f.png" alt="image-20231031052746310" class="img_ev3q"></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="llm-with-the-clip-onnx">LLM with the CLIP-ONNX<a href="#llm-with-the-clip-onnx" class="hash-link" aria-label="LLM with the CLIP-ONNXçš„ç›´æ¥é“¾æ¥" title="LLM with the CLIP-ONNXçš„ç›´æ¥é“¾æ¥">â€‹</a></h3>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="llm-with-the-captchaagent">LLM with the CaptchaAgent<a href="#llm-with-the-captchaagent" class="hash-link" aria-label="LLM with the CaptchaAgentçš„ç›´æ¥é“¾æ¥" title="LLM with the CaptchaAgentçš„ç›´æ¥é“¾æ¥">â€‹</a></h3></div><footer class="row docusaurus-mt-lg"><div class="col"><b>æ ‡ç­¾ï¼š</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/zh-Hans/blog/tags/hci-challenge">HCI challenge</a></li></ul></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="https://schema.org/BlogPosting"><meta itemprop="description" content="Milestone"><meta itemprop="keywords" content="hcaptcha,captcha,YOLOv8,object detection,image segmentation,bounding box,ONNX,hcaptca-challenger"><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/zh-Hans/blog/draw-a-tight-bounding-box-around-the-x">åœ¨ç›®æ ‡å‘¨å›´ç”»ä¸€ä¸ªè¾¹ç•Œæ¡†</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2023-08-28T00:00:00.000Z" itemprop="datePublished">2023å¹´8æœˆ28æ—¥</time> Â· <!-- -->é˜…è¯»éœ€ 1 åˆ†é’Ÿ</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://github.com/QIN2DIM" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://github.com/QIN2DIM.png" alt="QIN2DIM" itemprop="image"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/QIN2DIM" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">QIN2DIM</span></a></div><small class="avatar__subtitle" itemprop="description">Maintainer of CaptchaAgent</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><h2 class="anchor anchorWithStickyNavbar_LWe7" id="milestone">Milestone<a href="#milestone" class="hash-link" aria-label="Milestoneçš„ç›´æ¥é“¾æ¥" title="Milestoneçš„ç›´æ¥é“¾æ¥">â€‹</a></h2>
<p><a href="https://github.com/QIN2DIM/hcaptcha-challenger/issues/592" target="_blank" rel="noopener noreferrer">[Challenge] Draw a tight bounding box around the X  #592</a></p>
<p>Similar to the <a href="/zh-Hans/blog/please-click-on-the-x">point type challenge</a>, the principle of both similar challenges is object detection.</p>
<p>However, the output of the <code>bounding box</code> method changes from the coordinates of the center point of the bounding box to the start and end points.</p>
<div class="language-yaml codeBlockContainer_Ckt0 theme-code-block" style="--prism-background-color:hsl(230, 1%, 98%);--prism-color:hsl(230, 8%, 24%)"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-yaml codeBlock_bY9V thin-scrollbar" style="background-color:hsl(230, 1%, 98%);color:hsl(230, 8%, 24%)"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token key atrule" style="color:hsl(35, 99%, 36%)">prompt</span><span class="token punctuation" style="color:hsl(119, 34%, 47%)">:</span><span class="token plain"> Draw a tight bounding box around the guÑ–tĞ°r.</span><br></span><span class="token-line" style="color:hsl(230, 8%, 24%)"><span class="token plain"></span><span class="token key atrule" style="color:hsl(35, 99%, 36%)">type</span><span class="token punctuation" style="color:hsl(119, 34%, 47%)">:</span><span class="token plain"> bounding box</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="å¤åˆ¶ä»£ç åˆ°å‰ªè´´æ¿" title="å¤åˆ¶" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p><img loading="lazy" src="https://r2-datalake.echosec.top/blog-obs/2023/10/a8e4fd61370418f5e35ebdea07f45cba.png" alt="263652272-dbe5f4f3-c141-4e35-bbca-e20917408be9" class="img_ev3q"></p></div><footer class="row docusaurus-mt-lg"><div class="col"><b>æ ‡ç­¾ï¼š</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/zh-Hans/blog/tags/hci-challenge">HCI challenge</a></li></ul></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="https://schema.org/BlogPosting"><meta itemprop="description" content="You know Who I Am ğŸ¤–"><meta itemprop="keywords" content="hcaptcha,captcha,YOLOv8,object detection,Visual Question Answering,hcaptca-challenger"><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/zh-Hans/blog/bot-revolution-2023">æœºå™¨é©å‘½ 2023</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2023-05-15T00:00:00.000Z" itemprop="datePublished">2023å¹´5æœˆ15æ—¥</time> Â· <!-- -->é˜…è¯»éœ€ 1 åˆ†é’Ÿ</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://github.com/QIN2DIM" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://github.com/QIN2DIM.png" alt="QIN2DIM" itemprop="image"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/QIN2DIM" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">QIN2DIM</span></a></div><small class="avatar__subtitle" itemprop="description">Maintainer of CaptchaAgent</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><h2 class="anchor anchorWithStickyNavbar_LWe7" id="you-know-who-i-am-">You know Who I Am ğŸ¤–<a href="#you-know-who-i-am-" class="hash-link" aria-label="You know Who I Am ğŸ¤–çš„ç›´æ¥é“¾æ¥" title="You know Who I Am ğŸ¤–çš„ç›´æ¥é“¾æ¥">â€‹</a></h2>
<p><img loading="lazy" src="https://user-images.githubusercontent.com/62018067/225232056-b31da9d8-02a8-4bf0-adf6-fba10ed8bbb8.png" alt="image" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="related-challenges">Related Challenges<a href="#related-challenges" class="hash-link" aria-label="Related Challengesçš„ç›´æ¥é“¾æ¥" title="Related Challengesçš„ç›´æ¥é“¾æ¥">â€‹</a></h2>
<ul>
<li><a href="https://github.com/QIN2DIM/hcaptcha-challenger/issues/82" target="_blank" rel="noopener noreferrer">[*Challenge] image label area select #82</a></li>
<li><a href="https://github.com/QIN2DIM/hcaptcha-challenger/issues/176" target="_blank" rel="noopener noreferrer">[*Challenge] image label multiple choice #176</a></li>
<li><a href="https://github.com/QIN2DIM/hcaptcha-challenger/issues/236" target="_blank" rel="noopener noreferrer">[*Challenge] Draw a tight box around the toilet #236</a></li>
<li><a href="https://github.com/QIN2DIM/hcaptcha-challenger/issues/241" target="_blank" rel="noopener noreferrer">[*Challenge] Word Îœatching #241</a></li>
<li><a href="https://github.com/QIN2DIM/hcaptcha-challenger/issues/459" target="_blank" rel="noopener noreferrer">[*Challenge] Is the highlighted dot on the bigger squirrel? #459</a></li>
<li><a href="https://github.com/QIN2DIM/hcaptcha-challenger/issues/471" target="_blank" rel="noopener noreferrer">[*Challenge] center of the owl&#x27;s head #471</a></li>
<li><a href="https://github.com/QIN2DIM/hcaptcha-challenger/issues/479" target="_blank" rel="noopener noreferrer">[*Challenge] please click the center of the X #479</a></li>
</ul></div><footer class="row docusaurus-mt-lg"><div class="col"><b>æ ‡ç­¾ï¼š</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/zh-Hans/blog/tags/hci-challenge">HCI challenge</a></li></ul></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="https://schema.org/BlogPosting"><meta itemprop="description" content="How Do I Know If My Dog Is Smiling?"><meta itemprop="keywords" content="hcaptcha,captcha,hcaptca-challenger"><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/zh-Hans/blog/can-dogs-smile">ç§€ç‹—åœ¨ç¬‘ï¼Ÿ</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2022-09-22T00:00:00.000Z" itemprop="datePublished">2022å¹´9æœˆ22æ—¥</time> Â· <!-- -->é˜…è¯»éœ€ 1 åˆ†é’Ÿ</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://github.com/QIN2DIM" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://github.com/QIN2DIM.png" alt="QIN2DIM" itemprop="image"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/QIN2DIM" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">QIN2DIM</span></a></div><small class="avatar__subtitle" itemprop="description">Maintainer of CaptchaAgent</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p><img loading="lazy" src="https://r2-datalake.echosec.top/blog-obs/2023/11/beb1c7ad73b6372814691bbfd9e65603.png" alt="Snipaste_2023-10-31_06-34-58" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="tltd">TL;TD<a href="#tltd" class="hash-link" aria-label="TL;TDçš„ç›´æ¥é“¾æ¥" title="TL;TDçš„ç›´æ¥é“¾æ¥">â€‹</a></h2>
<p><strong>YES, BUT</strong> it is worth mentioning that the dog is not always &quot;smiling&quot; when it opens its mouth, they&#x27;re smiling because they are calm and relaxed, but sometimes it is also an expression of aggressive behavior.</p>
<p>In the challenge, it doesn&#x27;t really matter if the dog is smiling or not. Your ability to pass the challenge depends entirely on whether the person labeling the dataset thinks the dog is smiling.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="how-do-i-know-if-my-dog-is-smiling">How Do I Know If My Dog Is Smiling?<a href="#how-do-i-know-if-my-dog-is-smiling" class="hash-link" aria-label="How Do I Know If My Dog Is Smiling?çš„ç›´æ¥é“¾æ¥" title="How Do I Know If My Dog Is Smiling?çš„ç›´æ¥é“¾æ¥">â€‹</a></h2>
<p>The ASPCA noted, &quot;This is also a gesture where a dog shows his front teeth, but a smiling dog is doing just that. He usually shows a lowered head, wagging tail, flattened ears, a soft body posture and soft, squinty eyes along with those teeth. Teeth don&#x27;t always mean aggressionâ€”it is important to consider the whole body and the context to understand what a dog is saying.&quot;</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="reading">Reading<a href="#reading" class="hash-link" aria-label="Readingçš„ç›´æ¥é“¾æ¥" title="Readingçš„ç›´æ¥é“¾æ¥">â€‹</a></h2>
<ul>
<li><a href="https://www.hillspet.com/dog-care/behavior-appearance/can-dogs-smile" target="_blank" rel="noopener noreferrer">Can Dogs Actually Smile? | Hill&#x27;s Pet</a></li>
<li><a href="https://www.hcaptcha.com/post/can-dogs-smile" target="_blank" rel="noopener noreferrer">Can Dogs Smile? | Blog - hCaptcha</a></li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="related-work">Related work<a href="#related-work" class="hash-link" aria-label="Related workçš„ç›´æ¥é“¾æ¥" title="Related workçš„ç›´æ¥é“¾æ¥">â€‹</a></h2>
<ul>
<li><a href="https://github.com/QIN2DIM/hcaptcha-challenger/issues/87" target="_blank" rel="noopener noreferrer">[Challenge] smiling Ôog #87</a></li>
<li><a href="https://github.com/QIN2DIM/hcaptcha-challenger/issues/98" target="_blank" rel="noopener noreferrer">[Challenge] dog with closed eyes #98</a></li>
<li><a href="https://github.com/QIN2DIM/hcaptcha-challenger/issues/115" target="_blank" rel="noopener noreferrer">[Challenge] dog without a collar #115</a></li>
<li><a href="https://github.com/QIN2DIM/hcaptcha-challenger/issues/113" target="_blank" rel="noopener noreferrer">[Challenge] dog with a collar on its neck #113</a></li>
</ul></div><footer class="row docusaurus-mt-lg"><div class="col"><b>æ ‡ç­¾ï¼š</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/zh-Hans/blog/tags/hci-challenge">HCI challenge</a></li></ul></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="https://schema.org/BlogPosting"><meta itemprop="description" content="Cat Age Chart with Pictures"><meta itemprop="keywords" content="hcaptcha,captcha,hcaptca-challenger"><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/zh-Hans/blog/how-old-is-that-kitten">ä½ å®¶çŒ«çŒ«â€¦â€¦å‡ å²ï¼</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2022-08-30T00:00:00.000Z" itemprop="datePublished">2022å¹´8æœˆ30æ—¥</time> Â· <!-- -->é˜…è¯»éœ€ 1 åˆ†é’Ÿ</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://github.com/QIN2DIM" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://github.com/QIN2DIM.png" alt="QIN2DIM" itemprop="image"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/QIN2DIM" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">QIN2DIM</span></a></div><small class="avatar__subtitle" itemprop="description">Maintainer of CaptchaAgent</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><h2 class="anchor anchorWithStickyNavbar_LWe7" id="cat-age-chart-with-pictures">Cat Age Chart with Pictures<a href="#cat-age-chart-with-pictures" class="hash-link" aria-label="Cat Age Chart with Picturesçš„ç›´æ¥é“¾æ¥" title="Cat Age Chart with Picturesçš„ç›´æ¥é“¾æ¥">â€‹</a></h2>
<p><a href="https://www.alleycat.org/resources/kitten-progression/" target="_blank" rel="noopener noreferrer">Newborn Kitten Progression &amp; Cat Age Chart with Pictures | Alley Cat Allies</a></p>
<p><img loading="lazy" src="https://r2-datalake.echosec.top/blog-obs/2023/10/00a48bda62e3acbac8ea005075bb0c5f.jpg" alt="7d524620139047c35387c20e1e8ab9e2-1" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="related-work">Related work<a href="#related-work" class="hash-link" aria-label="Related workçš„ç›´æ¥é“¾æ¥" title="Related workçš„ç›´æ¥é“¾æ¥">â€‹</a></h2>
<ul>
<li><a href="https://github.com/QIN2DIM/hcaptcha-challenger/issues/163" target="_blank" rel="noopener noreferrer">[Challenge] adult cat #163</a></li>
<li><a href="https://github.com/QIN2DIM/hcaptcha-challenger/issues/158" target="_blank" rel="noopener noreferrer">[Challenge] cat with large, rounded head #158</a></li>
<li><a href="https://github.com/QIN2DIM/hcaptcha-challenger/issues/156" target="_blank" rel="noopener noreferrer">[Challenge] kitten #156</a></li>
<li><a href="https://github.com/QIN2DIM/hcaptcha-challenger/issues/150" target="_blank" rel="noopener noreferrer">[Challenge] cat with long hair #150</a></li>
<li><a href="https://github.com/QIN2DIM/hcaptcha-challenger/issues/148" target="_blank" rel="noopener noreferrer">[Challenge] cat with short hair #148</a></li>
<li><a href="https://github.com/QIN2DIM/hcaptcha-challenger/issues/146" target="_blank" rel="noopener noreferrer">[Challenge] cat with thick fur #146</a></li>
<li><a href="https://github.com/QIN2DIM/hcaptcha-challenger/issues/142" target="_blank" rel="noopener noreferrer">[Challenge] baby cat #142</a></li>
<li><a href="https://github.com/QIN2DIM/hcaptcha-challenger/issues/68" target="_blank" rel="noopener noreferrer">[Challenge] Domestic Cat #68</a></li>
</ul></div><footer class="row docusaurus-mt-lg"><div class="col"><b>æ ‡ç­¾ï¼š</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/zh-Hans/blog/tags/hci-challenge">HCI challenge</a></li></ul></div></footer></article><nav class="pagination-nav" aria-label="åšæ–‡åˆ—è¡¨åˆ†é¡µå¯¼èˆª"></nav></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright Â© 2023 CaptchaAgent. All right.</div></div></div></footer></div>
</body>
</html>