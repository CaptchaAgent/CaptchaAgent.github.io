"use strict";(self.webpackChunkCaptchaAgent=self.webpackChunkCaptchaAgent||[]).push([[5144],{3149:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>r,default:()=>l,frontMatter:()=>s,metadata:()=>a,toc:()=>d});var o=t(5893),i=t(1151);const s={title:"Discord GEN",description:"It's your concern."},r=void 0,a={id:"examples/integration-with-discord-gen",title:"Discord GEN",description:"It's your concern.",source:"@site/i18n/zh-Hans/docusaurus-plugin-content-docs/current/examples/05-integration-with-discord-gen.md",sourceDirName:"examples",slug:"/examples/integration-with-discord-gen",permalink:"/zh-Hans/docs/examples/integration-with-discord-gen",draft:!1,unlisted:!1,editUrl:"https://github.com/CaptchaAgent/docs-source/tree/main/docs/examples/05-integration-with-discord-gen.md",tags:[],version:"current",sidebarPosition:5,frontMatter:{title:"Discord GEN",description:"It's your concern."},sidebar:"tutorialSidebar",previous:{title:"Self-Supervised Challenge",permalink:"/zh-Hans/docs/examples/self-supervised-challenge"},next:{title:"Epic Games claimer",permalink:"/zh-Hans/docs/examples/integration-with-epic-claimer"}},c={},d=[{value:"Introduction",id:"introduction",level:2}];function u(e){const n={code:"code",h2:"h2",p:"p",...(0,i.a)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,o.jsx)(n.p,{children:"CaptchAgent enables you to run model inference tasks in a locally based environment. That is, you can perform ultra-high concurrency computer vision tasks (e.g., image classification, object detection, image segmentation) and you can process hundreds of images in seconds."}),"\n",(0,o.jsx)(n.p,{children:"For example, you can implement your own CAPTCHA-specific decoder without relying on any third-party solver service. So you don't need to share a queue with anyone, you don't need to wait tens or even hundreds of seconds for a queue, all processing is instantly responsive."}),"\n",(0,o.jsxs)(n.p,{children:["In addition, we packaged all models in ModelHub into ONNX format. At the same time, downstream tasks run inference tasks with ",(0,o.jsx)(n.code,{children:"onnxruntime"}),", meaning you can run AI models even if you don't have a GPU in your runtime environment."]})]})}function l(e={}){const{wrapper:n}={...(0,i.a)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(u,{...e})}):u(e)}},1151:(e,n,t)=>{t.d(n,{Z:()=>a,a:()=>r});var o=t(7294);const i={},s=o.createContext(i);function r(e){const n=o.useContext(s);return o.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),o.createElement(s.Provider,{value:n},e.children)}}}]);